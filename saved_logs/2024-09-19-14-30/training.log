model:
  target: models.unet.UNetModelSwin
  ckpt_path: logs/SinSR/2024-09-17-05-49/ema_ckpts/ema_model_16000.pth
  teacher_ckpt_path: weights/resshift_realsrx4_s15_v1.pth
  params:
    image_size: 64
    in_channels: 6
    model_channels: 160
    out_channels: 3
    cond_lq: true
    attention_resolutions:
    - 64
    - 32
    - 16
    - 8
    dropout: 0
    channel_mult:
    - 1
    - 2
    - 2
    - 4
    num_res_blocks:
    - 2
    - 2
    - 2
    - 2
    conv_resample: true
    dims: 2
    use_fp16: false
    num_head_channels: 32
    use_scale_shift_norm: true
    resblock_updown: false
    swin_depth: 2
    swin_embed_dim: 192
    window_size: 8
    mlp_ratio: 4
diffusion:
  target: models.script_util.create_gaussian_diffusion
  params:
    sf: 4
    schedule_name: exponential
    schedule_kwargs:
      power: 0.3
    etas_end: 0.99
    steps: 15
    min_noise_level: 0.04
    kappa: 2.0
    weighted_mse: false
    predict_type: xstart
    timestep_respacing: null
    scale_factor: 1.0
    normalize_input: true
    latent_flag: true
autoencoder:
  target: ldm.models.autoencoder.VQModelTorch
  ckpt_path: weights/autoencoder_vq_f4.pth
  use_fp16: true
  params:
    embed_dim: 3
    n_embed: 8192
    ddconfig:
      double_z: false
      z_channels: 3
      resolution: 256
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult:
      - 1
      - 2
      - 4
      num_res_blocks: 2
      attn_resolutions: []
      dropout: 0.0
      padding_mode: zeros
degradation:
  sf: 4
  resize_prob:
  - 0.2
  - 0.7
  - 0.1
  resize_range:
  - 0.15
  - 1.5
  gaussian_noise_prob: 0.5
  noise_range:
  - 1
  - 30
  poisson_scale_range:
  - 0.05
  - 3.0
  gray_noise_prob: 0.4
  jpeg_range:
  - 30
  - 95
  second_order_prob: 0.0
  second_blur_prob: 0.8
  resize_prob2:
  - 0.3
  - 0.4
  - 0.3
  resize_range2:
  - 0.3
  - 1.2
  gaussian_noise_prob2: 0.5
  noise_range2:
  - 1
  - 25
  poisson_scale_range2:
  - 0.05
  - 2.5
  gray_noise_prob2: 0.4
  jpeg_range2:
  - 30
  - 95
  gt_size: 256
  resize_back: false
  use_sharp: false
data:
  train:
    type: realesrgan
    params:
      root_path: /content
      dir_paths: []
      txt_file_path: []
      im_exts:
      - JPEG
      io_backend:
        type: disk
      blur_kernel_size: 21
      kernel_list:
      - iso
      - aniso
      - generalized_iso
      - generalized_aniso
      - plateau_iso
      - plateau_aniso
      kernel_prob:
      - 0.45
      - 0.25
      - 0.12
      - 0.03
      - 0.12
      - 0.03
      sinc_prob: 0.1
      blur_sigma:
      - 0.2
      - 3.0
      betag_range:
      - 0.5
      - 4.0
      betap_range:
      - 1
      - 2.0
      blur_kernel_size2: 15
      kernel_list2:
      - iso
      - aniso
      - generalized_iso
      - generalized_aniso
      - plateau_iso
      - plateau_aniso
      kernel_prob2:
      - 0.45
      - 0.25
      - 0.12
      - 0.03
      - 0.12
      - 0.03
      sinc_prob2: 0.1
      blur_sigma2:
      - 0.2
      - 1.5
      betag_range2:
      - 0.5
      - 4.0
      betap_range2:
      - 1
      - 2.0
      final_sinc_prob: 0.8
      gt_size: 256
      crop_pad_size: 300
      use_hflip: true
      use_rot: false
      rescale_gt: true
  val:
    type: tempoSpatial
    params:
      sf: 4
      root_path: /content
      dir_paths: []
      txt_file_path: []
      im_exts:
      - JPEG
      io_backend:
        type: disk
      transform_type: default
      transform_kwargs:
        mean: 0.5
        std: 0.5
      gt_size: 256
      rescale_gt: true
      resize_back: false
      matlab_mode: false
  experiment:
    type: tempoSpatial_Experiments
    params:
      root_path: /content
      blur_kernel_size: 21
      kernel_list:
      - iso
      - aniso
      - generalized_iso
      - generalized_aniso
      - plateau_iso
      - plateau_aniso
      kernel_prob:
      - 0.45
      - 0.25
      - 0.12
      - 0.03
      - 0.12
      - 0.03
      sinc_prob: 0.1
      blur_sigma:
      - 0.2
      - 3.0
      betag_range:
      - 0.5
      - 4.0
      betap_range:
      - 1
      - 2.0
      final_sinc_prob: 0.8
      gt_size: 256
      rescale_gt: true
train:
  learn_xT: true
  finetune_use_gt: 1.0
  lr: 5.0e-05
  batch:
  - 176
  - 8
  use_fp16: false
  microbatch: 44
  seed: 123456
  global_seeding: false
  prefetch_factor: 2
  num_workers: 2
  ema_rate: 0.999
  iterations: 500000
  milestones:
  - 1000
  - 50000
  weight_decay: 0
  save_freq: 500
  val_freq: 500
  log_freq:
  - 10
  - 5000
  - 5
  save_images: true
  use_ema_val: true
  test_baseline: true
experiment_evaluate:
  save_images: true
  save_path: experiment_evaluate_16000_bicubic_only
  batch: 80
  prefetch_factor: 8
  num_workers: 12
save_dir: ./saved_logs
resume: ''
cfg_path: configs/SinSR_eval.yaml

Initializing model from logs/SinSR/2024-09-17-05-49/ema_ckpts/ema_model_16000.pth
Restoring autoencoder from weights/autoencoder_vq_f4.pth
Detailed network architecture:
UNetModelSwin(
  (time_embed): Sequential(
    (0): Linear(in_features=160, out_features=640, bias=True)
    (1): SiLU()
    (2): Linear(in_features=640, out_features=640, bias=True)
  )
  (input_blocks): ModuleList(
    (0): TimestepEmbedSequential(
      (0): Conv2d(6, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (1): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 160, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=320, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 160, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Identity()
      )
      (1): BasicLayer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(160, 192, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (patch_unembed): PatchUnEmbed(
          (proj): Conv2d(192, 160, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=[64, 64], num_heads=6, window_size=8, shift_size=0, mlp_ratio=4
            (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (mlp): Mlp(
              (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=[64, 64], num_heads=6, window_size=8, shift_size=4, mlp_ratio=4
            (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (mlp): Mlp(
              (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0, inplace=False)
            )
          )
        )
      )
    )
    (2): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 160, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=320, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 160, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Identity()
      )
    )
    (3): TimestepEmbedSequential(
      (0): Downsample(
        (op): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (4): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 160, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=640, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): BasicLayer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(320, 192, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (patch_unembed): PatchUnEmbed(
          (proj): Conv2d(192, 320, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=[32, 32], num_heads=6, window_size=8, shift_size=0, mlp_ratio=4
            (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (mlp): Mlp(
              (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=[32, 32], num_heads=6, window_size=8, shift_size=4, mlp_ratio=4
            (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (mlp): Mlp(
              (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0, inplace=False)
            )
          )
        )
      )
    )
    (5): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=640, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Identity()
      )
    )
    (6): TimestepEmbedSequential(
      (0): Downsample(
        (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (7): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=640, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Identity()
      )
      (1): BasicLayer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(320, 192, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (patch_unembed): PatchUnEmbed(
          (proj): Conv2d(192, 320, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=[16, 16], num_heads=6, window_size=8, shift_size=0, mlp_ratio=4
            (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (mlp): Mlp(
              (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=[16, 16], num_heads=6, window_size=8, shift_size=4, mlp_ratio=4
            (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (mlp): Mlp(
              (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0, inplace=False)
            )
          )
        )
      )
    )
    (8): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=640, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Identity()
      )
    )
    (9): TimestepEmbedSequential(
      (0): Downsample(
        (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (10): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=1280, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): BasicLayer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(640, 192, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (patch_unembed): PatchUnEmbed(
          (proj): Conv2d(192, 640, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-1): 2 x SwinTransformerBlock(
            dim=192, input_resolution=[8, 8], num_heads=6, window_size=8, shift_size=0, mlp_ratio=4
            (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (mlp): Mlp(
              (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0, inplace=False)
            )
          )
        )
      )
    )
    (11): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=1280, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Identity()
      )
    )
  )
  (middle_block): TimestepEmbedSequential(
    (0): ResBlock(
      (in_layers): Sequential(
        (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=640, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0, inplace=False)
        (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Identity()
    )
    (1): BasicLayer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(640, 192, kernel_size=(1, 1), stride=(1, 1))
        (norm): Identity()
      )
      (patch_unembed): PatchUnEmbed(
        (proj): Conv2d(192, 640, kernel_size=(1, 1), stride=(1, 1))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          dim=192, input_resolution=[8, 8], num_heads=6, window_size=8, shift_size=0, mlp_ratio=4
          (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(8, 8), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
          (mlp): Mlp(
            (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
            (act): GELU(approximate='none')
            (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (2): ResBlock(
      (in_layers): Sequential(
        (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (h_upd): Identity()
      (x_upd): Identity()
      (emb_layers): Sequential(
        (0): SiLU()
        (1): Linear(in_features=640, out_features=1280, bias=True)
      )
      (out_layers): Sequential(
        (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
        (1): SiLU()
        (2): Dropout(p=0, inplace=False)
        (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (skip_connection): Identity()
    )
  )
  (output_blocks): ModuleList(
    (0): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=1280, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): BasicLayer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(640, 192, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (patch_unembed): PatchUnEmbed(
          (proj): Conv2d(192, 640, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-1): 2 x SwinTransformerBlock(
            dim=192, input_resolution=[8, 8], num_heads=6, window_size=8, shift_size=0, mlp_ratio=4
            (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (mlp): Mlp(
              (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0, inplace=False)
            )
          )
        )
      )
    )
    (1): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=1280, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (2): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=1280, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): Upsample(
        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (3): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 960, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=640, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): BasicLayer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(320, 192, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (patch_unembed): PatchUnEmbed(
          (proj): Conv2d(192, 320, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=[16, 16], num_heads=6, window_size=8, shift_size=0, mlp_ratio=4
            (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (mlp): Mlp(
              (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=[16, 16], num_heads=6, window_size=8, shift_size=4, mlp_ratio=4
            (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (mlp): Mlp(
              (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0, inplace=False)
            )
          )
        )
      )
    )
    (4): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=640, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (5): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=640, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): Upsample(
        (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (6): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=640, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): BasicLayer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(320, 192, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (patch_unembed): PatchUnEmbed(
          (proj): Conv2d(192, 320, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=[32, 32], num_heads=6, window_size=8, shift_size=0, mlp_ratio=4
            (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (mlp): Mlp(
              (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=[32, 32], num_heads=6, window_size=8, shift_size=4, mlp_ratio=4
            (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (mlp): Mlp(
              (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0, inplace=False)
            )
          )
        )
      )
    )
    (7): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 640, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=640, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (8): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 480, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(480, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=640, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(480, 320, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): Upsample(
        (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (9): TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 480, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(480, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=320, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 160, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): BasicLayer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(160, 192, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (patch_unembed): PatchUnEmbed(
          (proj): Conv2d(192, 160, kernel_size=(1, 1), stride=(1, 1))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=[64, 64], num_heads=6, window_size=8, shift_size=0, mlp_ratio=4
            (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (mlp): Mlp(
              (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=[64, 64], num_heads=6, window_size=8, shift_size=4, mlp_ratio=4
            (norm1): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): GroupNorm32(32, 192, eps=1e-05, affine=True)
            (mlp): Mlp(
              (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))
              (act): GELU(approximate='none')
              (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0, inplace=False)
            )
          )
        )
      )
    )
    (10-11): 2 x TimestepEmbedSequential(
      (0): ResBlock(
        (in_layers): Sequential(
          (0): GroupNorm32(32, 320, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Conv2d(320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (h_upd): Identity()
        (x_upd): Identity()
        (emb_layers): Sequential(
          (0): SiLU()
          (1): Linear(in_features=640, out_features=320, bias=True)
        )
        (out_layers): Sequential(
          (0): GroupNorm32(32, 160, eps=1e-05, affine=True)
          (1): SiLU()
          (2): Dropout(p=0, inplace=False)
          (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (skip_connection): Conv2d(320, 160, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (out): Sequential(
    (0): GroupNorm32(32, 160, eps=1e-05, affine=True)
    (1): SiLU()
    (2): Conv2d(160, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
Number of parameters: 118.59M
Number of images in train data set: 1000000000
Number of images in expr data set: 1000000000
Number of images in val data set: 0
